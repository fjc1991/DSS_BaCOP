{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Final_Data.csv\",encoding='latin')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['id','url','description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting total text number\n",
    "\n",
    "df1['count_description'] = df1['description'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "df1['description'] = df1['description'].dropna()\n",
    "df1['description'] = df1.description.astype(str)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list()\n",
    "lines = df1['description'].values.tolist()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean text\n",
    "def review_to_words(raw_review):\n",
    "    \n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) \n",
    "    \n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 3. Remove Stopwords. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]  #returns a list \n",
    "\n",
    "    # 5. Stem words. Need to define porter stemmer above\n",
    "    singles = [stemmer.stem(word) for word in meaningful_words]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join( singles ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['clean_description'] = [review_to_words(text) for text in df1['description']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer & Pos Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toktok = ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['clean_description'] = df1['clean_description'].apply(lambda x: toktok.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['description_tag'] = df1['clean_description'].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvCounter(x):\n",
    "    adv = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"RB\"):\n",
    "            adv.append(word)\n",
    "    return adv\n",
    "\n",
    "def Adv_com_Counter(x):\n",
    "    adv = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"RBR\"):\n",
    "            adv.append(word)\n",
    "    return adv\n",
    "\n",
    "def Adv_sup_Counter(x):\n",
    "    adv = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"RBS\"):\n",
    "            adv.append(word)\n",
    "    return adv\n",
    "def Noun_Counter(x):\n",
    "    nn = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"NN\"):\n",
    "            nn.append(word)\n",
    "    return nn\n",
    "def Noun_plural_Counter(x):\n",
    "    nn = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"NNS\"):\n",
    "            nn.append(word)\n",
    "    return nn\n",
    "def Noun_proper_Counter(x):\n",
    "    nn = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"NNP\"):\n",
    "            nn.append(word)\n",
    "    return nn\n",
    "def Noun_proper_plural_Counter(x):\n",
    "    nn = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"NNPS\"):\n",
    "            nn.append(word)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"descrip_adv\"] = df1[\"description_tag\"].apply(AdvCounter)\n",
    "df1[\"descrip_adv_count\"] = df1[\"descrip_adv\"].str.len()\n",
    "df1[\"descrip_adv_com\"] = df1[\"description_tag\"].apply(Adv_com_Counter)\n",
    "df1[\"descrip_adv_com_count\"] = df1[\"descrip_adv_com\"].str.len()\n",
    "df1[\"descrip_adv_sup\"] = df1[\"description_tag\"].apply(Adv_sup_Counter)\n",
    "df1[\"descrip_adv_sup_count\"] = df1[\"descrip_adv_sup\"].str.len()\n",
    "df1[\"descrip_noun\"] = df1[\"description_tag\"].apply(Noun_Counter)\n",
    "df1[\"descrip_noun_count\"] = df1[\"descrip_noun\"].str.len()\n",
    "df1[\"descrip_noun_plu\"] = df1[\"description_tag\"].apply(Noun_plural_Counter)\n",
    "df1[\"descrip_noun_plu_count\"] = df1[\"descrip_noun_plu\"].str.len()\n",
    "df1[\"descrip_noun_pro\"] = df1[\"description_tag\"].apply(Noun_proper_Counter)\n",
    "df1[\"descrip_noun_pro_count\"] = df1[\"descrip_noun_pro\"].str.len()\n",
    "df1[\"descrip_noun_pro_plu\"] = df1[\"description_tag\"].apply(Noun_proper_plural_Counter)\n",
    "df1[\"descrip_noun_pro__plu_count\"] = df1[\"descrip_noun_pro_plu\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Verb_Counter(x):\n",
    "    vb = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"VB\"):\n",
    "            vb.append(word)\n",
    "    return vb\n",
    "def Verb_past_Counter(x):\n",
    "    vb = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"VBD\"):\n",
    "            vb.append(word)\n",
    "    return vb\n",
    "def Verb_pres_Counter(x):\n",
    "    vb = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"VBG\"):\n",
    "            vb.append(word)\n",
    "    return vb\n",
    "def Verb_past_part_Counter(x):\n",
    "    vb = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"VBN\"):\n",
    "            vb.append(word)\n",
    "    return vb\n",
    "def Verb_sing_Counter(x):\n",
    "    vb = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"VBP\"):\n",
    "            vb.append(word)\n",
    "    return vb\n",
    "def Verb_third_Counter(x):\n",
    "    vb = []\n",
    "    for (word, pos) in x:\n",
    "        if pos.startswith(\"VBZ\"):\n",
    "            vb.append(word)\n",
    "    return vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"descrip_verb\"] = df1[\"description_tag\"].apply(Verb_Counter)\n",
    "df1[\"descrip_verb_count\"] = df1[\"descrip_verb\"].str.len()\n",
    "df1[\"descrip_verb_past\"] = df1[\"description_tag\"].apply(Verb_past_Counter)\n",
    "df1[\"descrip_verb_past_count\"] = df1[\"descrip_verb_past\"].str.len()\n",
    "df1[\"descrip_verb_pres\"] = df1[\"description_tag\"].apply(Verb_pres_Counter)\n",
    "df1[\"descrip_verb_pres_count\"] = df1[\"descrip_verb_pres\"].str.len()\n",
    "df1[\"descrip_verb_past_part\"] = df1[\"description_tag\"].apply(Verb_past_part_Counter)\n",
    "df1[\"descrip_verb_past_part_count\"] = df1[\"descrip_verb_past_part\"].str.len()\n",
    "df1[\"descrip_verb_sing\"] = df1[\"description_tag\"].apply(Verb_sing_Counter)\n",
    "df1[\"descrip_verb_sing_count\"] = df1[\"descrip_verb_sing\"].str.len()\n",
    "df1[\"descrip_verb_third\"] = df1[\"description_tag\"].apply(Verb_third_Counter)\n",
    "df1[\"descrip_verb_third_count\"] = df1[\"descrip_verb_third\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"new_postag_des.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['clean_description'] = df1['clean_description'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sentiment analysis\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_score_list = []\n",
    "sentiment_label_list = []\n",
    "\n",
    "for i in df1['clean_description'].values.tolist():\n",
    "    sentiment_score = analyser.polarity_scores(i)\n",
    "\n",
    "    if sentiment_score['compound'] >= 0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Positive')\n",
    "    elif sentiment_score['compound'] > -0.05 and sentiment_score['compound'] < 0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Neutral')\n",
    "    elif sentiment_score['compound'] <= -0.05:\n",
    "        sentiment_score_list.append(sentiment_score['compound'])\n",
    "        sentiment_label_list.append('Negative')\n",
    "    \n",
    "df1['sentiment'] = sentiment_label_list\n",
    "df1['sentiment score'] = sentiment_score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat.textstat import textstat\n",
    "\n",
    "df1['Flesch_RC'] = df1['description'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "\n",
    "# df1['Smog_RC'] = df1['description'].apply(lambda x: textstat.smog_index(x))\n",
    "\n",
    "df1['Word_count'] = df1['description'].apply(lambda x: textstat.lexicon_count(x, removepunct=True))\n",
    "\n",
    "df1['Sent_count'] = df1['description'].apply(lambda x: textstat.sentence_count(x))\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vagueness Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import functools\n",
    "import re\n",
    "\n",
    "def import_txtfile_set(file_path):\n",
    "    with open(file_path) as t_file:\n",
    "        return set([line.strip().lower() for line in t_file.readlines()])\n",
    "\n",
    "def idf(documents, selected_words):\n",
    "    words = {}\n",
    "    just_selected = []\n",
    "    for sentence in documents:\n",
    "        for word in sentence.split():\n",
    "            if word in selected_words:\n",
    "                just_selected.append(word)\n",
    "    for word in just_selected:\n",
    "        if word in words:\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "\n",
    "    print(words)\n",
    "    for key, value in words.items():\n",
    "        words[key] = math.log(len(just_selected) / value) if  value != 0 else 0\n",
    "    return words\n",
    "\n",
    "def get_frequency_scores(sentence, search_words, stop_words, word_idf):\n",
    "    words = [x.strip().lower() for x in str(sentence).split()]\n",
    "    clean_words = [word for word in words if word not in stop_words]\n",
    "    matching_words = [word for word in clean_words if word in search_words]\n",
    "    try:\n",
    "        return functools.reduce(lambda total, word: word_idf[word], matching_words, 0) / len(words)*100\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Input All field names in the data to be analysed\n",
    "fields = ['description']\n",
    "\n",
    "model_dict = 'dictionaries/'\n",
    "models = os.listdir(model_dict)\n",
    "\n",
    "\n",
    "for field in fields:\n",
    "    df_lda[field + '_clean'] = df_lda[field].apply(lambda text: re.sub(r'[?.!,;\\-_\\/\\'\\\"]', ' ', str(text)))\n",
    "\n",
    "    for model in models:\n",
    "        model_path = model_dict + model + '/'\n",
    "        stopwords = import_txtfile_set(model_path + 'stopwords.txt')\n",
    "        for category in os.listdir(model_path + 'categories/'):\n",
    "            category_path = model_path + 'categories' + '/'\n",
    "            search_words = import_txtfile_set(category_path + category)\n",
    "            field_idf = idf(list(df_lda[field + '_clean']), search_words)\n",
    "            df_lda[f'{model}_{category}_frequency'] = df_lda[field + '_clean'].apply(lambda sentence: get_frequency_scores(sentence, search_words, stopwords, field_idf))\n",
    "\n",
    "# data.to_csv('Dataset_with_ambiguity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda.to_csv('Description_lda.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
